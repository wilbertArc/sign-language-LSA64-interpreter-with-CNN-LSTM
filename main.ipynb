{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abf784a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install opencv-python\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5d17a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d714a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes: ['001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064']\n",
      "Num classes: 64\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"dataset\"\n",
    "\n",
    "# find all mp4 files\n",
    "files = sorted(glob(os.path.join(DATASET_DIR, \"*.mp4\")))\n",
    "\n",
    "# extract class label prefix (first 3 digits)\n",
    "classes = sorted(list(set([os.path.basename(f)[:3] for f in files])))\n",
    "\n",
    "print(\"Detected classes:\", classes)\n",
    "print(\"Num classes:\", len(classes))\n",
    "\n",
    "gloss_to_idx = {cls: i for i, cls in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "309862c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignDataset(Dataset):\n",
    "    def __init__(self, video_folder, gloss_to_idx, num_frames=16, augment=False):\n",
    "        self.folder = video_folder\n",
    "        self.gloss_to_idx = gloss_to_idx\n",
    "        self.num_frames = num_frames\n",
    "        self.augment = augment\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize((112,112))\n",
    "        ])\n",
    "\n",
    "        # create samples list\n",
    "        self.samples = []\n",
    "        videos = sorted(glob(os.path.join(video_folder, \"*.mp4\")))\n",
    "\n",
    "        for v in videos:\n",
    "            basename = os.path.basename(v)\n",
    "            cls = basename[:3]     # extract class\n",
    "            if cls in gloss_to_idx:\n",
    "                self.samples.append({\n",
    "                    \"path\": v,\n",
    "                    \"label\": gloss_to_idx[cls]\n",
    "                })\n",
    "\n",
    "        print(\"Loaded\", len(self.samples), \"videos\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def load_video(self, path):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # handle broken videos\n",
    "        if len(frames) == 0:\n",
    "            return torch.zeros(self.num_frames, 3, 112, 112)\n",
    "\n",
    "        # sample frames\n",
    "        idxs = np.linspace(0, len(frames)-1, self.num_frames).astype(int)\n",
    "\n",
    "        processed = []\n",
    "        for i in idxs:\n",
    "            frame = frames[i]\n",
    "\n",
    "            # ðŸ”¥ Correct preprocessing\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (112,112))\n",
    "            \n",
    "            # Data augmentation for training\n",
    "            if self.augment:\n",
    "                # Random horizontal flip\n",
    "                if np.random.rand() > 0.4:\n",
    "                    frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Random brightness/contrast adjustment\n",
    "                brightness = np.random.uniform(0.7, 1.3)\n",
    "                frame = np.clip(frame * brightness, 0, 255).astype(np.uint8)\n",
    "                \n",
    "                # Random rotation (small angle)\n",
    "                if np.random.rand() > 0.5:\n",
    "                    angle = np.random.uniform(-10, 10)\n",
    "                    h, w = frame.shape[:2]\n",
    "                    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "                    frame = cv2.warpAffine(frame, M, (w, h))\n",
    "            \n",
    "            frame = torch.from_numpy(frame).permute(2,0,1).float() / 255.0\n",
    "\n",
    "            processed.append(frame)\n",
    "\n",
    "        return torch.stack(processed)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        path = item[\"path\"]\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            print(\"MISSING VIDEO:\", path)\n",
    "            raise FileNotFoundError(path)\n",
    "\n",
    "        frames = self.load_video(path)\n",
    "        return frames, item[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8af602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3200 videos\n",
      "Loaded 3200 videos\n",
      "Train: 2560\n",
      "Val: 320\n",
      "Test: 320\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = SignDataset(DATASET_DIR, gloss_to_idx)\n",
    "\n",
    "indices = np.arange(len(dataset))\n",
    "\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.2, shuffle=True)\n",
    "val_idx, test_idx   = train_test_split(temp_idx, test_size=0.5, shuffle=True)\n",
    "\n",
    "# Create datasets with augmentation for training\n",
    "train_ds = SignDataset(DATASET_DIR, gloss_to_idx, augment=True)\n",
    "train_ds = torch.utils.data.Subset(train_ds, train_idx)\n",
    "\n",
    "val_ds   = torch.utils.data.Subset(dataset, val_idx)\n",
    "test_ds  = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,   # <-- REQUIRED on Windows\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train:\", len(train_ds))\n",
    "print(\"Val:\", len(val_ds))\n",
    "print(\"Test:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7448d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        base = mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "        base.classifier = nn.Identity()\n",
    "        self.cnn = base\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1280,\n",
    "            hidden_size=512,\n",
    "            batch_first=True,\n",
    "            dropout=0.6\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.reshape(B*T, C, H, W)\n",
    "\n",
    "        feat = self.cnn(x)\n",
    "        feat = feat.reshape(B, T, 1280)\n",
    "\n",
    "        out, _ = self.lstm(feat)\n",
    "        out = out[:, -1]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ef2af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CNN_LSTM(num_classes=len(classes)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c63bc5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [28:51<00:00,  2.71s/it]\n",
      "Epoch 1 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [02:58<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.5545 | Acc: 0.1605 | Val Loss: 2.4993 | Val Acc: 0.4375\n",
      "--> Saved new best model with Val Loss: 2.4993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [28:58<00:00,  2.72s/it]\n",
      "Epoch 2 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [03:00<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1046 | Acc: 0.5156 | Val Loss: 1.3094 | Val Acc: 0.7438\n",
      "--> Saved new best model with Val Loss: 1.3094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [30:08<00:00,  2.83s/it]\n",
      "Epoch 3 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [03:09<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1622 | Acc: 0.7605 | Val Loss: 0.6880 | Val Acc: 0.8812\n",
      "--> Saved new best model with Val Loss: 0.6880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [30:31<00:00,  2.86s/it]\n",
      "Epoch 4 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [03:06<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6531 | Acc: 0.8914 | Val Loss: 0.3382 | Val Acc: 0.9437\n",
      "--> Saved new best model with Val Loss: 0.3382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [31:15<00:00,  2.93s/it]\n",
      "Epoch 5 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [03:11<00:00,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4198 | Acc: 0.9336 | Val Loss: 0.2472 | Val Acc: 0.9656\n",
      "--> Saved new best model with Val Loss: 0.2472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "PATIENCE = 2\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "import json\n",
    "with open(\"class_names.json\", \"w\") as f:\n",
    "    json.dump(classes, f)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    train_loss = 0\n",
    "\n",
    "    for frames, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_loss_avg = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    val_loss_avg = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss_avg:.4f} | Acc: {train_acc:.4f} | Val Loss: {val_loss_avg:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    # --- Early stopping & Saving ---\n",
    "    if val_loss_avg < best_val_loss:\n",
    "        best_val_loss = val_loss_avg\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # ðŸ”¥ ADD THESE LINES TO SAVE THE DATA\n",
    "        # 1. Save the model weights (the \"brain\")\n",
    "        torch.save(model.state_dict(), \"best_sign_model.pth\")\n",
    "        \n",
    "        # 2. Save the class names (the \"labels\")\n",
    "        import json\n",
    "        with open(\"class_names.json\", \"w\") as f:\n",
    "            json.dump(classes, f)\n",
    "            \n",
    "        print(f\"--> Saved new best model with Val Loss: {val_loss_avg:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c1d7c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [03:09<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for frames, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(frames)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc088c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'class_names.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Load the classes first\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclass_names.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      6\u001b[39m     loaded_classes = json.load(f)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2. Re-create the model structure\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:327\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'class_names.json'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "# 1. Load the classes first\n",
    "with open(\"class_names.json\", \"r\") as f:\n",
    "    loaded_classes = json.load(f)\n",
    "\n",
    "# 2. Re-create the model structure\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNN_LSTM(num_classes=len(loaded_classes)).to(device)\n",
    "\n",
    "# 3. Load the saved \"brain\" (weights)\n",
    "model.load_state_dict(torch.load(\"best_sign_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully! You can now use it for predictions.\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
